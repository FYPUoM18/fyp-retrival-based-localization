{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bGIUFul3pCLR",
    "outputId": "cb80e6c3-16a9-436c-d55c-28e7528cdbf4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "TiFk_kqmo78Q"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import h5py\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "VgMwyv2W5_K6"
   },
   "outputs": [],
   "source": [
    "# Load the RoNIN data\n",
    "X = []  # list to store the data points\n",
    "y = []  # list to store the class labels\n",
    "\n",
    "with h5py.File(f\"/content/drive/MyDrive/fypResearchDatasets/a000_9.hdf5\", \"r\") as h5f:\n",
    "    # Extract the data from the \"synced\" group\n",
    "    data = h5f[\"synced\"]\n",
    "    time = data[\"time\"][:]\n",
    "    time = time.reshape(-1, 1)  # shape (N, 1)\n",
    "    gyro = data[\"gyro\"][:]\n",
    "    gyro_uncalib = data[\"gyro_uncalib\"][:]\n",
    "    acce = data[\"acce\"][:]\n",
    "    linacce = data[\"linacce\"][:]\n",
    "    magnet = data[\"magnet\"][:]\n",
    "    rv = data[\"rv\"][:]\n",
    "    game_rv = data[\"game_rv\"][:]\n",
    "    \n",
    "    # Extract the labels from the \"pose\" group\n",
    "    labels = h5f[\"pose\"][\"tango_pos\"][:, 2]  # use the z-coordinate as the label\n",
    "    \n",
    "    # Store the data and labels in the lists\n",
    "    X.append(np.concatenate([time, gyro, gyro_uncalib, acce, linacce, magnet, rv, game_rv], axis=1))\n",
    "    y.append(labels)\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "TjX7rMWA6B5B"
   },
   "outputs": [],
   "source": [
    "# Convert the lists to NumPy arrays\n",
    "X = np.concatenate(X, axis=0)\n",
    "y = np.concatenate(y, axis=0)\n",
    "\n",
    "# Split the data into a training set and a test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "YKDp0eUO-hEp"
   },
   "outputs": [],
   "source": [
    "# Split the data into a training set and a test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "Bsz2ho8R-k7w"
   },
   "outputs": [],
   "source": [
    "# Convert the data to PyTorch tensors\n",
    "X_train = torch.from_numpy(X_train).float()\n",
    "X_test = torch.from_numpy(X_test).float()\n",
    "y_train = torch.from_numpy(y_train).long()\n",
    "y_test = torch.from_numpy(y_test).long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "EM7AGRiz-pdu"
   },
   "outputs": [],
   "source": [
    "# Define the contrastive loss function\n",
    "class ContrastiveLoss(torch.nn.Module):\n",
    "    def __init__(self, margin=1.0):\n",
    "        super().__init__()\n",
    "        self.margin = margin\n",
    "        \n",
    "    def forward(self, output1, output2, label):\n",
    "        distance = torch.nn.functional.pairwise_distance(output1, output2)\n",
    "        loss = torch.mean((1 - label) * distance ** 2 + label * torch.clamp(self.margin - distance, min=0) ** 2)\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "--pg8piG_Kk9"
   },
   "outputs": [],
   "source": [
    "# Define the neural network\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super().__init__()\n",
    "        self.fc1 = torch.nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = torch.nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "foe7HHCxAkWK"
   },
   "outputs": [],
   "source": [
    "# Create an instance of the Net class\n",
    "net = Net(input_size=X_train.shape[1], hidden_size=32, output_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "YZDKe7v2Anym"
   },
   "outputs": [],
   "source": [
    "# Define the optimizer and criterion\n",
    "optimizer = torch.optim.Adam(net.parameters())\n",
    "criterion = ContrastiveLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6oXCdIqPAq7D",
    "outputId": "5ffe9c42-3c83-4c3e-9f86-6661279c6abe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/250: loss = 0.000014\n",
      "Epoch 2/250: loss = 0.000000\n",
      "Epoch 3/250: loss = 0.000000\n",
      "Epoch 4/250: loss = 0.000000\n",
      "Epoch 5/250: loss = 0.000003\n",
      "Epoch 6/250: loss = 0.000015\n",
      "Epoch 7/250: loss = 0.000001\n",
      "Epoch 8/250: loss = 0.000002\n",
      "Epoch 9/250: loss = 0.000025\n",
      "Epoch 10/250: loss = 0.000003\n",
      "Epoch 11/250: loss = 0.000009\n",
      "Epoch 12/250: loss = 0.000025\n",
      "Epoch 13/250: loss = 0.000004\n",
      "Epoch 14/250: loss = 0.000002\n",
      "Epoch 15/250: loss = 0.000010\n",
      "Epoch 16/250: loss = 0.000041\n",
      "Epoch 17/250: loss = 0.000024\n",
      "Epoch 18/250: loss = 0.000000\n",
      "Epoch 19/250: loss = 0.000002\n",
      "Epoch 20/250: loss = 0.000038\n",
      "Epoch 21/250: loss = 0.000000\n",
      "Epoch 22/250: loss = 0.000000\n",
      "Epoch 23/250: loss = 0.000004\n",
      "Epoch 24/250: loss = 0.000005\n",
      "Epoch 25/250: loss = 0.000000\n",
      "Epoch 26/250: loss = 0.000000\n",
      "Epoch 27/250: loss = 0.000000\n",
      "Epoch 28/250: loss = 0.000004\n",
      "Epoch 29/250: loss = 0.000025\n",
      "Epoch 30/250: loss = 0.000000\n",
      "Epoch 31/250: loss = 0.000001\n",
      "Epoch 32/250: loss = 0.000017\n",
      "Epoch 33/250: loss = 0.000000\n",
      "Epoch 34/250: loss = 0.000007\n",
      "Epoch 35/250: loss = 0.000003\n",
      "Epoch 36/250: loss = 0.000000\n",
      "Epoch 37/250: loss = 0.000000\n",
      "Epoch 38/250: loss = 0.000016\n",
      "Epoch 39/250: loss = 0.000007\n",
      "Epoch 40/250: loss = 0.000002\n",
      "Epoch 41/250: loss = 0.000004\n",
      "Epoch 42/250: loss = 0.000001\n",
      "Epoch 43/250: loss = 0.000008\n",
      "Epoch 44/250: loss = 0.000000\n",
      "Epoch 45/250: loss = 0.000010\n",
      "Epoch 46/250: loss = 0.000004\n",
      "Epoch 47/250: loss = 0.000002\n",
      "Epoch 48/250: loss = 0.000016\n",
      "Epoch 49/250: loss = 0.000012\n",
      "Epoch 50/250: loss = 0.000000\n",
      "Epoch 51/250: loss = 0.000002\n",
      "Epoch 52/250: loss = 0.000006\n",
      "Epoch 53/250: loss = 0.000000\n",
      "Epoch 54/250: loss = 0.000022\n",
      "Epoch 55/250: loss = 0.000000\n",
      "Epoch 56/250: loss = 0.000000\n",
      "Epoch 57/250: loss = 0.000007\n",
      "Epoch 58/250: loss = 0.000022\n",
      "Epoch 59/250: loss = 0.000017\n",
      "Epoch 60/250: loss = 0.000000\n",
      "Epoch 61/250: loss = 0.000002\n",
      "Epoch 62/250: loss = 0.000010\n",
      "Epoch 63/250: loss = 0.000018\n",
      "Epoch 64/250: loss = 0.000011\n",
      "Epoch 65/250: loss = 0.000031\n",
      "Epoch 66/250: loss = 0.000001\n",
      "Epoch 67/250: loss = 0.000017\n",
      "Epoch 68/250: loss = 0.000000\n",
      "Epoch 69/250: loss = 0.000000\n",
      "Epoch 70/250: loss = 0.000002\n",
      "Epoch 71/250: loss = 0.000015\n",
      "Epoch 72/250: loss = 0.000001\n",
      "Epoch 73/250: loss = 0.000000\n",
      "Epoch 74/250: loss = 0.000000\n",
      "Epoch 75/250: loss = 0.000000\n",
      "Epoch 76/250: loss = 0.000003\n",
      "Epoch 77/250: loss = 0.000004\n",
      "Epoch 78/250: loss = 0.000000\n",
      "Epoch 79/250: loss = 0.000011\n",
      "Epoch 80/250: loss = 0.000001\n",
      "Epoch 81/250: loss = 0.000000\n",
      "Epoch 82/250: loss = 0.000018\n",
      "Epoch 83/250: loss = 0.000000\n",
      "Epoch 84/250: loss = 0.000000\n",
      "Epoch 85/250: loss = 0.000029\n",
      "Epoch 86/250: loss = 0.000006\n",
      "Epoch 87/250: loss = 0.000001\n",
      "Epoch 88/250: loss = 0.000001\n",
      "Epoch 89/250: loss = 0.000000\n",
      "Epoch 90/250: loss = 0.000000\n",
      "Epoch 91/250: loss = 0.000005\n",
      "Epoch 92/250: loss = 0.000003\n",
      "Epoch 93/250: loss = 0.000003\n",
      "Epoch 94/250: loss = 0.000017\n",
      "Epoch 95/250: loss = 0.000000\n",
      "Epoch 96/250: loss = 0.000008\n",
      "Epoch 97/250: loss = 0.000010\n",
      "Epoch 98/250: loss = 0.000000\n",
      "Epoch 99/250: loss = 0.000008\n",
      "Epoch 100/250: loss = 0.000018\n",
      "Epoch 101/250: loss = 0.000018\n",
      "Epoch 102/250: loss = 0.000000\n",
      "Epoch 103/250: loss = 0.000010\n",
      "Epoch 104/250: loss = 0.000035\n",
      "Epoch 105/250: loss = 0.000003\n",
      "Epoch 106/250: loss = 0.000000\n",
      "Epoch 107/250: loss = 0.000000\n",
      "Epoch 108/250: loss = 0.000003\n",
      "Epoch 109/250: loss = 0.000009\n",
      "Epoch 110/250: loss = 0.000006\n",
      "Epoch 111/250: loss = 0.000036\n",
      "Epoch 112/250: loss = 0.000004\n",
      "Epoch 113/250: loss = 0.000039\n",
      "Epoch 114/250: loss = 0.000010\n",
      "Epoch 115/250: loss = 0.000007\n",
      "Epoch 116/250: loss = 0.000004\n",
      "Epoch 117/250: loss = 0.000004\n",
      "Epoch 118/250: loss = 0.000000\n",
      "Epoch 119/250: loss = 0.000028\n",
      "Epoch 120/250: loss = 0.000018\n",
      "Epoch 121/250: loss = 0.000007\n",
      "Epoch 122/250: loss = 0.000005\n",
      "Epoch 123/250: loss = 0.000003\n",
      "Epoch 124/250: loss = 0.000023\n",
      "Epoch 125/250: loss = 0.000011\n",
      "Epoch 126/250: loss = 0.000004\n",
      "Epoch 127/250: loss = 0.000001\n",
      "Epoch 128/250: loss = 0.000002\n",
      "Epoch 129/250: loss = 0.000014\n",
      "Epoch 130/250: loss = 0.000019\n",
      "Epoch 131/250: loss = 0.000007\n",
      "Epoch 132/250: loss = 0.000028\n",
      "Epoch 133/250: loss = 0.000026\n",
      "Epoch 134/250: loss = 0.000000\n",
      "Epoch 135/250: loss = 0.000035\n",
      "Epoch 136/250: loss = 0.000016\n",
      "Epoch 137/250: loss = 0.000019\n",
      "Epoch 138/250: loss = 0.000028\n",
      "Epoch 139/250: loss = 0.000000\n",
      "Epoch 140/250: loss = 0.000000\n",
      "Epoch 141/250: loss = 0.000011\n",
      "Epoch 142/250: loss = 0.000001\n",
      "Epoch 143/250: loss = 0.000001\n",
      "Epoch 144/250: loss = 0.000000\n",
      "Epoch 145/250: loss = 0.000001\n",
      "Epoch 146/250: loss = 0.000001\n",
      "Epoch 147/250: loss = 0.000000\n",
      "Epoch 148/250: loss = 0.000000\n",
      "Epoch 149/250: loss = 0.000012\n",
      "Epoch 150/250: loss = 0.000013\n",
      "Epoch 151/250: loss = 0.000002\n",
      "Epoch 152/250: loss = 0.000001\n",
      "Epoch 153/250: loss = 0.000000\n",
      "Epoch 154/250: loss = 0.000026\n",
      "Epoch 155/250: loss = 0.000022\n",
      "Epoch 156/250: loss = 0.000014\n",
      "Epoch 157/250: loss = 0.000000\n",
      "Epoch 158/250: loss = 0.000015\n",
      "Epoch 159/250: loss = 0.000002\n",
      "Epoch 160/250: loss = 0.000003\n",
      "Epoch 161/250: loss = 0.000000\n",
      "Epoch 162/250: loss = 0.000000\n",
      "Epoch 163/250: loss = 0.000022\n",
      "Epoch 164/250: loss = 0.000018\n",
      "Epoch 165/250: loss = 0.000003\n",
      "Epoch 166/250: loss = 0.000000\n",
      "Epoch 167/250: loss = 0.000003\n",
      "Epoch 168/250: loss = 0.000023\n",
      "Epoch 169/250: loss = 0.000001\n",
      "Epoch 170/250: loss = 0.000000\n",
      "Epoch 171/250: loss = 0.000027\n",
      "Epoch 172/250: loss = 0.000000\n",
      "Epoch 173/250: loss = 0.000003\n",
      "Epoch 174/250: loss = 0.000002\n",
      "Epoch 175/250: loss = 0.000000\n",
      "Epoch 176/250: loss = 0.000000\n",
      "Epoch 177/250: loss = 0.000000\n",
      "Epoch 178/250: loss = 0.000000\n",
      "Epoch 179/250: loss = 0.000013\n",
      "Epoch 180/250: loss = 0.000000\n",
      "Epoch 181/250: loss = 0.000012\n",
      "Epoch 182/250: loss = 0.000021\n",
      "Epoch 183/250: loss = 0.000014\n",
      "Epoch 184/250: loss = 0.000020\n",
      "Epoch 185/250: loss = 0.000005\n",
      "Epoch 186/250: loss = 0.000000\n",
      "Epoch 187/250: loss = 0.000009\n",
      "Epoch 188/250: loss = 0.000004\n",
      "Epoch 189/250: loss = 0.000017\n",
      "Epoch 190/250: loss = 0.000000\n",
      "Epoch 191/250: loss = 0.000000\n",
      "Epoch 192/250: loss = 0.000011\n",
      "Epoch 193/250: loss = 0.000000\n",
      "Epoch 194/250: loss = 0.000028\n",
      "Epoch 195/250: loss = 0.000000\n",
      "Epoch 196/250: loss = 0.000019\n",
      "Epoch 197/250: loss = 0.000000\n",
      "Epoch 198/250: loss = 0.000001\n",
      "Epoch 199/250: loss = 0.000000\n",
      "Epoch 200/250: loss = 0.000005\n",
      "Epoch 201/250: loss = 0.000016\n",
      "Epoch 202/250: loss = 0.000000\n",
      "Epoch 203/250: loss = 0.000016\n",
      "Epoch 204/250: loss = 0.000000\n",
      "Epoch 205/250: loss = 0.000013\n",
      "Epoch 206/250: loss = 0.000006\n",
      "Epoch 207/250: loss = 0.000000\n",
      "Epoch 208/250: loss = 0.000003\n",
      "Epoch 209/250: loss = 0.000000\n",
      "Epoch 210/250: loss = 0.000003\n",
      "Epoch 211/250: loss = 0.000013\n",
      "Epoch 212/250: loss = 0.000006\n",
      "Epoch 213/250: loss = 0.000000\n",
      "Epoch 214/250: loss = 0.000000\n",
      "Epoch 215/250: loss = 0.000000\n",
      "Epoch 216/250: loss = 0.000000\n",
      "Epoch 217/250: loss = 0.000000\n",
      "Epoch 218/250: loss = 0.000002\n",
      "Epoch 219/250: loss = 0.000001\n",
      "Epoch 220/250: loss = 0.000000\n",
      "Epoch 221/250: loss = 0.000000\n",
      "Epoch 222/250: loss = 0.000005\n",
      "Epoch 223/250: loss = 0.000002\n",
      "Epoch 224/250: loss = 0.000032\n",
      "Epoch 225/250: loss = 0.000015\n",
      "Epoch 226/250: loss = 0.000000\n",
      "Epoch 227/250: loss = 0.000013\n",
      "Epoch 228/250: loss = 0.000017\n",
      "Epoch 229/250: loss = 0.000000\n",
      "Epoch 230/250: loss = 0.000000\n",
      "Epoch 231/250: loss = 0.000023\n",
      "Epoch 232/250: loss = 0.000047\n",
      "Epoch 233/250: loss = 0.000002\n",
      "Epoch 234/250: loss = 0.000003\n",
      "Epoch 235/250: loss = 0.000002\n",
      "Epoch 236/250: loss = 0.000000\n",
      "Epoch 237/250: loss = 0.000000\n",
      "Epoch 238/250: loss = 0.000000\n",
      "Epoch 239/250: loss = 0.000000\n",
      "Epoch 240/250: loss = 0.000000\n",
      "Epoch 241/250: loss = 0.000017\n",
      "Epoch 242/250: loss = 0.000025\n",
      "Epoch 243/250: loss = 0.000000\n",
      "Epoch 244/250: loss = 0.000009\n",
      "Epoch 245/250: loss = 0.000014\n",
      "Epoch 246/250: loss = 0.000002\n",
      "Epoch 247/250: loss = 0.000001\n",
      "Epoch 248/250: loss = 0.000000\n",
      "Epoch 249/250: loss = 0.000000\n",
      "Epoch 250/250: loss = 0.000004\n",
      "Test accuracy: 1.00\n"
     ]
    }
   ],
   "source": [
    "# Train the network using contrastive learning\n",
    "num_epochs = 250\n",
    "for epoch in range(num_epochs):\n",
    "    # Shuffle the training data\n",
    "    indices = torch.randperm(X_train.shape[0])\n",
    "    X_train = X_train[indices]\n",
    "    y_train = y_train[indices]\n",
    "    \n",
    "    # Split the training data into pairs\n",
    "    X1 = X_train[::2]\n",
    "    X2 = X_train[1::2]\n",
    "    y1 = y_train[::2]\n",
    "    y2 = y_train[1::2]\n",
    "    \n",
    "    # Compute the features for each pair of data points\n",
    "    output1 = net(X1)\n",
    "    output2 = net(X2)\n",
    "    \n",
    "    # Compute the contrastive loss for each pair of data points\n",
    "    loss = criterion(output1, output2, (y1 == y2).long())\n",
    "    \n",
    "    # Backpropagate the loss and update the weights\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Print the training loss\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}: loss = {loss.item():.6f}\")\n",
    "    \n",
    "# Compute the features for the test data\n",
    "features = net(X_test)\n",
    "\n",
    "# Convert the features to a NumPy array\n",
    "features = features.detach().numpy()\n",
    "\n",
    "# Train a KNN classifier using the features\n",
    "knn = KNeighborsClassifier()\n",
    "knn.fit(features, y_test)\n",
    "\n",
    "\n",
    "# Test the KNN classifier on the test data\n",
    "accuracy = knn.score(features, y_test)\n",
    "print(f\"Test accuracy: {accuracy:.2f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "nsd8ssseBJ-l"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
